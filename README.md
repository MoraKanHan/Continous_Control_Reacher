# Continous_Control_Reacher
For this project, I trained an agent to move one doubled-joined arm to target locations. The environment has state space of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. The agent generates an action vector with four numbers, corresponding to torque applicable to two joints. The agent gains a reward of +0.1 for each step that the agent's hand is in the goal location. The aim of the agent is to maximize reward, thus, maintain its position at the target location for as many time steps as possible. The task is solved when the average score is 30 for 100 consecutive episodes. I used two algorithms to solve this problem. The first algorithm used was vanilla Deep Deterministic Policy Gradient (DDPG) and the second algorithm used is a DDPG with prioritized replay buffer. 
